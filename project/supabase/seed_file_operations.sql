/*
  # PyLearn File Operations Lessons

  ## Overview
  10 comprehensive lessons covering Python file operations
  From basic file reading/writing to advanced file system operations

  ## Structure
  - Basic File Operations (3): Reading, writing, file paths
  - File Management (3): Working with directories, file operations
  - Advanced File Operations (3): Context managers, error handling, file formats
  - Practical Applications (1): Real-world file processing project

  ## Target Audience
  Beginners to intermediate learners mastering file I/O operations
*/

-- First, let's get the section ID for file operations
-- The section should already exist in the database schema

-- Basic File Operations (3 lessons)
INSERT INTO lessons (id, title, description, difficulty, xp_reward, order_index, section_id, content, lesson_type, estimated_minutes) VALUES
('80000000-0000-0000-0000-000000000001', 'Reading Text Files', 'Learn how to read data from text files', 'beginner', 15, 1,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "Reading files is essential for working with data. Python provides simple ways to read text files."}, {"type": "text", "content": "Basic file reading methods:"}, {"type": "text", "content": "1. read() - reads entire file as string"}, {"type": "text", "content": "2. readline() - reads one line at a time"}, {"type": "text", "content": "3. readlines() - reads all lines into a list"}, {"type": "text", "content": "Always use the with statement for file operations - it automatically closes the file!"}, {"type": "code", "question": "Read a file and count lines", "starterCode": "def count_lines(filename):\n    # Read file and count lines\n    pass", "solution": "def count_lines(filename):\n    with open(filename, 'r') as file:\n        lines = file.readlines()\n        return len(lines)\n\n# Alternative method\nwith open(filename, 'r') as file:\n    line_count = sum(1 for line in file)\n    return line_count"}]',
'coding', 15),

('80000000-0000-0000-0000-000000000002', 'Writing to Text Files', 'Learn how to write data to text files', 'beginner', 18, 2,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "Writing files allows you to save data and results permanently."}, {"type": "text", "content": "File writing modes:"}, {"type": "text", "content": "'w' - write mode (overwrites existing file)"}, {"type": "text", "content": "'a' - append mode (adds to end of file)"}, {"type": "text", "content": "'x' - exclusive creation mode (fails if file exists)"}, {"type": "code", "question": "Create a log file with timestamps", "starterCode": "import datetime\n\ndef write_log(message, filename=\"log.txt\"):\n    # Add timestamp and write to file\n    pass", "solution": "import datetime\n\ndef write_log(message, filename=\"log.txt\"):\n    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log_entry = f\"[{timestamp}] {message}\\n\"\n    \n    with open(filename, 'a') as file:\n        file.write(log_entry)\n\n# Test the function\nwrite_log(\"Application started\")\nwrite_log(\"User logged in\")\nwrite_log(\"Operation completed\")"}]',
'coding', 18),

('80000000-0000-0000-0000-000000000003', 'Working with File Paths', 'Master file path operations and directory navigation', 'beginner', 20, 3,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "File paths tell Python where to find files. Use os.path for cross-platform compatibility."}, {"type": "text", "content": "Key path operations:"}, {"type": "text", "content": "• os.path.join() - combine path parts safely"}, {"type": "text", "content": "• os.path.exists() - check if path exists"}, {"type": "text", "content": "• os.path.dirname() - get directory name"}, {"type": "text", "content": "• os.path.basename() - get file name"}, {"type": "code", "question": "Create function to handle file paths safely", "starterCode": "import os\n\ndef safe_file_operation(directory, filename, content):\n    # Handle paths safely and write file\n    pass", "solution": "import os\n\ndef safe_file_operation(directory, filename, content):\n    # Create full path safely\n    full_path = os.path.join(directory, filename)\n    \n    # Create directory if it doesn't exist\n    os.makedirs(directory, exist_ok=True)\n    \n    # Write content to file\n    with open(full_path, 'w') as file:\n        file.write(content)\n    \n    return full_path\n\n# Example usage\npath = safe_file_operation(\"data\", \"output.txt\", \"Hello, World!\")\nprint(f\"File written to: {path}\")"}]',
'coding', 20),

-- File Management (3 lessons)
INSERT INTO lessons (id, title, description, difficulty, xp_reward, order_index, section_id, content, lesson_type, estimated_minutes) VALUES
('80000000-0000-0000-0000-000000000004', 'Directory Operations', 'Working with directories and folder structures', 'intermediate', 22, 4,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "Directories help organize files. Python provides tools for directory management."}, {"type": "text", "content": "Essential directory operations:"}, {"type": "text", "content": "• os.listdir() - list files in directory"}, {"type": "text", "content": "• os.mkdir() - create single directory"}, {"type": "text", "content": "• os.makedirs() - create directory tree"}, {"type": "text", "content": "• os.rmdir() - remove empty directory"}, {"type": "text", "content": "• shutil.rmtree() - remove directory with contents"}, {"type": "code", "question": "Create a project structure generator", "starterCode": "import os\n\ndef create_project_structure(project_name, folders):\n    # Create project with given folders\n    pass", "solution": "import os\n\ndef create_project_structure(project_name, folders):\n    # Create main project directory\n    os.makedirs(project_name, exist_ok=True)\n    \n    # Create subdirectories\n    for folder in folders:\n        folder_path = os.path.join(project_name, folder)\n        os.makedirs(folder_path, exist_ok=True)\n        print(f\"Created: {folder_path}\")\n    \n    return f\"Project '{project_name}' created successfully!\"\n\n# Example usage\nfolders = ['src', 'tests', 'docs', 'data']\nresult = create_project_structure(\"my_app\", folders)\nprint(result)"}]',
'coding', 22),

('80000000-0000-0000-0000-000000000005', 'File Information and Metadata', 'Getting file details like size, modification time', 'intermediate', 25, 5,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "Files contain metadata like size, creation time, and permissions."}, {"type": "text", "content": "Use os.path and os.stat() to get file information:"}, {"type": "text", "content": "• os.path.getsize() - file size in bytes"}, {"type": "text", "content": "• os.path.getmtime() - modification timestamp"}, {"type": "text", "content": "• os.path.getctime() - creation timestamp"}, {"type": "text", "content": "• os.stat() - detailed file statistics"}, {"type": "code", "question": "Create file information analyzer", "starterCode": "import os\nimport datetime\n\ndef get_file_info(filename):\n    # Return file information dictionary\n    pass", "solution": "import os\nimport datetime\n\ndef get_file_info(filename):\n    if not os.path.exists(filename):\n        return {\"error\": f\"File '{filename}' not found\"}\n    \n    # Get file statistics\n    stat_info = os.stat(filename)\n    \n    # Format dates\n    created = datetime.datetime.fromtimestamp(stat_info.st_ctime)\n    modified = datetime.datetime.fromtimestamp(stat_info.st_mtime)\n    \n    return {\n        \"filename\": os.path.basename(filename),\n        \"directory\": os.path.dirname(filename),\n        \"size_bytes\": stat_info.st_size,\n        \"size_mb\": round(stat_info.st_size / (1024 * 1024), 2),\n        \"created\": created.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"modified\": modified.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"is_file\": os.path.isfile(filename),\n        \"is_directory\": os.path.isdir(filename)\n    }\n\n# Example usage\ninfo = get_file_info(\"example.txt\")\nfor key, value in info.items():\n    print(f\"{key}: {value}\")"}]',
'coding', 25),

('80000000-0000-0000-0000-000000000006', 'File Copying and Moving', 'Copying, moving, and renaming files safely', 'intermediate', 28, 6,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "The shutil module provides high-level file operations."}, {"type": "text", "content": "Common file operations:"}, {"type": "text", "content": "• shutil.copy() - copy file (preserves metadata)"}, {"type": "text", "content": "• shutil.copy2() - copy with all metadata"}, {"type": "text", "content": "• shutil.move() - move/rename file or directory"}, {"type": "text", "content": "• os.rename() - rename file or directory"}, {"type": "text", "content": "Always check if destination exists to avoid overwriting!"}, {"type": "code", "question": "Create safe file backup system", "starterCode": "import os\nimport shutil\nimport datetime\n\ndef backup_file(source_path, backup_dir=\"backups\"):\n    # Create timestamped backup\n    pass", "solution": "import os\nimport shutil\nimport datetime\n\ndef backup_file(source_path, backup_dir=\"backups\"):\n    if not os.path.exists(source_path):\n        return {\"error\": f\"Source file '{source_path}' not found\"}\n    \n    # Create backup directory\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    # Generate backup filename with timestamp\n    filename = os.path.basename(source_path)\n    name, ext = os.path.splitext(filename)\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_filename = f\"{name}_{timestamp}{ext}\"\n    \n    # Create backup path\n    backup_path = os.path.join(backup_dir, backup_filename)\n    \n    # Copy file with metadata\n    shutil.copy2(source_path, backup_path)\n    \n    return {\n        \"source\": source_path,\n        \"backup\": backup_path,\n        \"timestamp\": datetime.datetime.now().isoformat(),\n        \"size\": os.path.getsize(backup_path)\n    }\n\n# Example usage\nresult = backup_file(\"important_file.txt\")\nprint(f\"Backup created: {result['backup']}\")"}]',
'coding', 28),

-- Advanced File Operations (3 lessons)
INSERT INTO lessons (id, title, description, difficulty, xp_reward, order_index, section_id, content, lesson_type, estimated_minutes) VALUES
('80000000-0000-0000-0000-000000000007', 'Context Managers for File Handling', 'Using with statements for robust file operations', 'intermediate', 30, 7,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "Context managers (with statements) ensure proper resource cleanup."}, {"type": "text", "content": "Benefits of context managers:"}, {"type": "text", "content": "• Automatic file closing"}, {"type": "text", "content": "• Exception safety"}, {"type": "text", "content": "• Cleaner, more readable code"}, {"type": "text", "content": "• Prevents resource leaks"}, {"type": "code", "question": "Create custom context manager for file processing", "starterCode": "class FileProcessor:\n    def __init__(self, filename, mode='r'):\n        self.filename = filename\n        self.mode = mode\n        self.file = None\n    \n    def __enter__(self):\n        # Open file and return self\n        pass\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Close file and handle exceptions\n        pass", "solution": "class FileProcessor:\n    def __init__(self, filename, mode='r'):\n        self.filename = filename\n        self.mode = mode\n        self.file = None\n    \n    def __enter__(self):\n        self.file = open(self.filename, self.mode)\n        return self.file\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.file:\n            self.file.close()\n        \n        # If exception occurred, log it\n        if exc_type:\n            print(f\"Error processing {self.filename}: {exc_val}\")\n        \n        # Return False to re-raise exceptions, True to suppress\n        return False\n\n# Usage example\nwith FileProcessor(\"data.txt\", 'r') as file:\n    content = file.read()\n    print(f\"Read {len(content)} characters\")\n    \n# File is automatically closed"}]',
'coding', 30),

('80000000-0000-0000-0000-000000000008', 'Error Handling in File Operations', 'Handling file-related exceptions gracefully', 'intermediate', 32, 8,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "File operations can fail for many reasons. Proper error handling is crucial."}, {"type": "text", "content": "Common file exceptions:"}, {"type": "text", "content": "• FileNotFoundError - file doesn't exist"}, {"type": "text", "content": "• PermissionError - insufficient permissions"}, {"type": "text", "content": "• IsADirectoryError - trying to open directory as file"}, {"type": "text", "content": "• NotADirectoryError - path exists but isn't directory"}, {"type": "text", "content": "• OSError - general operating system error"}, {"type": "code", "question": "Create robust file reader with error handling", "starterCode": "def safe_read_file(filename, max_retries=3):\n    # Read file with retry logic and error handling\n    pass", "solution": "import time\nimport os\n\ndef safe_read_file(filename, max_retries=3):\n    \"\"\"Safely read a file with error handling and retries.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            # Check if file exists\n            if not os.path.exists(filename):\n                return {\"error\": f\"File '{filename}' does not exist\"}\n            \n            # Check if it's a file (not directory)\n            if not os.path.isfile(filename):\n                return {\"error\": f\"'{filename}' is not a file\"}\n            \n            # Try to read the file\n            with open(filename, 'r', encoding='utf-8') as file:\n                content = file.read()\n                return {\n                    \"success\": True,\n                    \"content\": content,\n                    \"size\": len(content),\n                    \"lines\": content.count('\\n') + 1,\n                    \"filename\": filename\n                }\n                \n        except FileNotFoundError:\n            return {\"error\": f\"File '{filename}' not found\"}\n        except PermissionError:\n            return {\"error\": f\"Permission denied accessing '{filename}'\"}\n        except UnicodeDecodeError:\n            return {\"error\": f\"Could not decode '{filename}' as UTF-8 text\"}\n        except OSError as e:\n            if attempt < max_retries - 1:\n                print(f\"Attempt {attempt + 1} failed, retrying in 1 second...\")\n                time.sleep(1)\n                continue\n            else:\n                return {\"error\": f\"Failed to read '{filename}': {str(e)}\"}\n    \n    return {\"error\": f\"Failed after {max_retries} attempts\"}\n\n# Test the function\nresult = safe_read_file(\"example.txt\")\nif \"error\" in result:\n    print(f\"Error: {result['error']}\")\nelse:\n    print(f\"Successfully read {result['size']} characters\")"}]',
'coding', 32),

('80000000-0000-0000-0000-000000000009', 'Working with Different File Formats', 'Reading and writing CSV, JSON, and other formats', 'intermediate', 35, 9,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "Python excels at handling various file formats beyond plain text."}, {"type": "text", "content": "Common file formats:"}, {"type": "text", "content": "• CSV - Comma-separated values (csv module)"}, {"type": "text", "content": "• JSON - JavaScript Object Notation (json module)"}, {"type": "text", "content": "• XML - eXtensible Markup Language (xml module)"}, {"type": "text", "content": "• YAML - YAML Ain't Markup Language (PyYAML)"}, {"type": "code", "question": "Create file format converter", "starterCode": "import json\nimport csv\nimport os\n\ndef convert_file_format(input_file, output_file, input_format, output_format):\n    # Convert between JSON, CSV, and plain text\n    pass", "solution": "import json\nimport csv\nimport os\n\ndef convert_file_format(input_file, output_file, input_format, output_format):\n    \"\"\"Convert between different file formats.\"\"\"\n    if not os.path.exists(input_file):\n        return {\"error\": f\"Input file '{input_file}' not found\"}\n    \n    try:\n        # Read input file\n        if input_format.lower() == 'json':\n            with open(input_file, 'r') as file:\n                data = json.load(file)\n        \n        elif input_format.lower() == 'csv':\n            data = []\n            with open(input_file, 'r', newline='') as file:\n                reader = csv.DictReader(file)\n                data = list(reader)\n        \n        elif input_format.lower() == 'txt':\n            with open(input_file, 'r') as file:\n                lines = file.readlines()\n                data = [{'line_number': i+1, 'content': line.strip()} \n                       for i, line in enumerate(lines) if line.strip()]\n        \n        else:\n            return {\"error\": f\"Unsupported input format: {input_format}\"}\n        \n        # Write output file\n        if output_format.lower() == 'json':\n            with open(output_file, 'w') as file:\n                json.dump(data, file, indent=2)\n        \n        elif output_format.lower() == 'csv' and data:\n            if data and isinstance(data, list) and isinstance(data[0], dict):\n                with open(output_file, 'w', newline='') as file:\n                    writer = csv.DictWriter(file, fieldnames=data[0].keys())\n                    writer.writeheader()\n                    writer.writerows(data)\n        \n        elif output_format.lower() == 'txt':\n            with open(output_file, 'w') as file:\n                if isinstance(data, list):\n                    for item in data:\n                        file.write(f\"{item}\\n\")\n                else:\n                    file.write(str(data))\n        \n        else:\n            return {\"error\": f\"Unsupported output format: {output_format}\"}\n        \n        return {\n            \"success\": True,\n            \"input_file\": input_file,\n            \"output_file\": output_file,\n            \"input_format\": input_format,\n            \"output_format\": output_format\n        }\n        \n    except Exception as e:\n        return {\"error\": f\"Conversion failed: {str(e)}\"}\n\n# Example usage\nresult = convert_file_format(\"data.json\", \"data.csv\", \"json\", \"csv\")\nprint(result)"}]',
'coding', 35),

-- Practical Applications (1 lesson)
INSERT INTO lessons (id, title, description, difficulty, xp_reward, order_index, section_id, content, lesson_type, estimated_minutes) VALUES
('80000000-0000-0000-0000-000000000010', 'File Processing Project', 'Build a real-world file processing application', 'intermediate', 40, 10,
 (SELECT id FROM sections WHERE path = 'file-operations'),
'[{"type": "text", "content": "Apply your file operations knowledge to build a practical file processing tool."}, {"type": "text", "content": "Project: Create a log file analyzer that processes application logs and generates reports."}, {"type": "text", "content": "Features to implement:"}, {"type": "text", "content": "• Parse log entries with different formats"}, {"type": "text", "content": "• Filter logs by level (INFO, ERROR, WARNING)"}, {"type": "text", "content": "• Generate summary statistics"}, {"type": "text", "content": "• Export results to different formats"}, {"type": "code", "question": "Build comprehensive log file analyzer", "starterCode": "import re\nimport os\nfrom datetime import datetime\nfrom collections import defaultdict, Counter\n\nclass LogAnalyzer:\n    def __init__(self):\n        self.log_patterns = {\n            'apache': r'(\\S+) - - \\[([^\\]]+)\\] \"(\\S+) (\\S+) (\\S+)\" (\\d+) (\\d+)',\n            'nginx': r'(\\S+) - - \\[([^\\]]+)\\] \"(\\S+) (\\S+) (\\S+)\" (\\d+) (\\S+)',\n            'python': r'\\[([^\\]]+)\\] (\\w+): (.*)'\n        }\n    \n    def parse_log_file(self, filename):\n        # Parse log file and return structured data\n        pass", "solution": "import re\nimport os\nfrom datetime import datetime\nfrom collections import defaultdict, Counter\nimport json\n\nclass LogAnalyzer:\n    def __init__(self):\n        self.log_patterns = {\n            'apache': r'(\\S+) - - \\[([^\\]]+)\\] \"(\\S+) (\\S+) (\\S+)\" (\\d+) (\\d+)',\n            'nginx': r'(\\S+) - - \\[([^\\]]+)\\] \"(\\S+) (\\S+) (\\S+)\" (\\d+) (\\S+)',\n            'python': r'\\[([^\\]]+)\\] (\\w+): (.*)'\n        }\n        self.parsed_logs = []\n    \n    def parse_log_file(self, filename, log_format='python'):\n        \"\"\"Parse log file and return structured data.\"\"\"\n        if not os.path.exists(filename):\n            return {\"error\": f\"Log file '{filename}' not found\"}\n        \n        pattern = self.log_patterns.get(log_format)\n        if not pattern:\n            return {\"error\": f\"Unsupported log format: {log_format}\"}\n        \n        regex = re.compile(pattern)\n        parsed_entries = []\n        \n        try:\n            with open(filename, 'r', encoding='utf-8') as file:\n                for line_num, line in enumerate(file, 1):\n                    line = line.strip()\n                    if not line:\n                        continue\n                    \n                    match = regex.match(line)\n                    if match:\n                        if log_format == 'python':\n                            timestamp, level, message = match.groups()\n                            entry = {\n                                'timestamp': timestamp,\n                                'level': level.upper(),\n                                'message': message,\n                                'line_number': line_num\n                            }\n                        else:  # Apache/Nginx\n                            ip, timestamp, method, path, protocol, status, size = match.groups()\n                            entry = {\n                                'ip': ip,\n                                'timestamp': timestamp,\n                                'method': method,\n                                'path': path,\n                                'protocol': protocol,\n                                'status': int(status),\n                                'size': int(size) if size.isdigit() else 0,\n                                'line_number': line_num\n                            }\n                        \n                        parsed_entries.append(entry)\n                    else:\n                        # Unmatched line\n                        parsed_entries.append({\n                            'raw_line': line,\n                            'line_number': line_num,\n                            'parsed': False\n                        })\n        \n        except Exception as e:\n            return {\"error\": f\"Error parsing log file: {str(e)}\"}\n        \n        self.parsed_logs = parsed_entries\n        return {\n            \"success\": True,\n            \"total_lines\": parsed_entries.__len__(),\n            \"parsed_entries\": len([e for e in parsed_entries if e.get('parsed', True)]),\n            \"entries\": parsed_entries\n        }\n    \n    def generate_statistics(self):\n        \"\"\"Generate statistics from parsed logs.\"\"\"\n        if not self.parsed_logs:\n            return {\"error\": \"No log data available. Parse a file first.\"}\n        \n        stats = {\n            'total_entries': len(self.parsed_logs),\n            'successful_parses': len([e for e in self.parsed_logs if e.get('parsed', True)]),\n            'failed_parses': len([e for e in self.parsed_logs if not e.get('parsed', True)])\n        }\n        \n        # Analyze log levels (for Python logs)\n        levels = Counter([entry.get('level') for entry in self.parsed_logs if entry.get('level')])\n        stats['log_levels'] = dict(levels)\n        \n        # Analyze HTTP status codes (for Apache/Nginx logs)\n        status_codes = Counter([entry.get('status') for entry in self.parsed_logs if entry.get('status')])\n        stats['status_codes'] = dict(status_codes)\n        \n        # Analyze IPs (for web logs)\n        ips = Counter([entry.get('ip') for entry in self.parsed_logs if entry.get('ip')])\n        stats['top_ips'] = dict(ips.most_common(10))\n        \n        return stats\n    \n    def filter_logs(self, level=None, status_code=None, ip=None, start_time=None, end_time=None):\n        \"\"\"Filter logs based on criteria.\"\"\"\n        filtered = []\n        \n        for entry in self.parsed_logs:\n            # Skip unparsed entries\n            if not entry.get('parsed', True):\n                continue\n            \n            # Filter by log level\n            if level and entry.get('level') != level.upper():\n                continue\n            \n            # Filter by HTTP status code\n            if status_code and entry.get('status') != status_code:\n                continue\n            \n            # Filter by IP address\n            if ip and entry.get('ip') != ip:\n                continue\n            \n            filtered.append(entry)\n        \n        return filtered\n    \n    def export_results(self, output_file, format='json'):\n        \"\"\"Export analysis results to file.\"\"\"\n        results = {\n            'analysis_timestamp': datetime.now().isoformat(),\n            'statistics': self.generate_statistics(),\n            'sample_entries': self.parsed_logs[:10]  # First 10 entries as sample\n        }\n        \n        try:\n            if format.lower() == 'json':\n                with open(output_file, 'w') as file:\n                    json.dump(results, file, indent=2)\n            \n            elif format.lower() == 'txt':\n                with open(output_file, 'w') as file:\n                    file.write(\"Log Analysis Report\\n\")\n                    file.write(\"=\" * 50 + \"\\n\\n\")\n                    file.write(f\"Analysis Date: {results['analysis_timestamp']}\\n\\n\")\n                    file.write(f\"Total Entries: {results['statistics']['total_entries']}\\n\")\n                    file.write(f\"Successful Parses: {results['statistics']['successful_parses']}\\n\")\n                    \n                    if results['statistics'].get('log_levels'):\n                        file.write(\"\\nLog Levels:\\n\")\n                        for level, count in results['statistics']['log_levels'].items():\n                            file.write(f\"  {level}: {count}\\n\")\n            \n            return {\"success\": True, \"output_file\": output_file}\n        \n        except Exception as e:\n            return {\"error\": f\"Export failed: {str(e)}\"}\n\n# Example usage\nif __name__ == \"__main__\":\n    analyzer = LogAnalyzer()\n    \n    # Parse log file\n    result = analyzer.parse_log_file(\"app.log\", \"python\")\n    print(\"Parsing result:\", result)\n    \n    if \"success\" in result:\n        # Generate statistics\n        stats = analyzer.generate_statistics()\n        print(\"\\nStatistics:\", stats)\n        \n        # Filter errors\n        errors = analyzer.filter_logs(level=\"ERROR\")\n        print(f\"\\nFound {len(errors)} error entries\")\n        \n        # Export results\n        export_result = analyzer.export_results(\"log_analysis.json\", \"json\")\n        print(\"\\nExport result:\", export_result)"}]',
'project', 45);

-- Create indexes for performance
CREATE INDEX IF NOT EXISTS idx_file_lessons_section ON lessons(section_id);
CREATE INDEX IF NOT EXISTS idx_file_lessons_order ON lessons(order_index);

-- Verify data was inserted
SELECT
  section_id,
  COUNT(*) as lesson_count,
  STRING_AGG(DISTINCT lesson_type, ', ') as lesson_types
FROM lessons
WHERE section_id = (SELECT id FROM sections WHERE path = 'file-operations')
GROUP BY section_id;

SELECT
  'Total File Operations Lessons Created' as metric,
  COUNT(*) as count
FROM lessons
WHERE section_id = (SELECT id FROM sections WHERE path = 'file-operations');